{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('database.txt')\n",
    "\n",
    "lines = file.readlines()\n",
    "url = lines[0].rstrip()\n",
    "driver = lines[1].rstrip()\n",
    "user = lines[2].rstrip()\n",
    "password = lines[3].rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('CreditCardSystem').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "brnch_df = spark.read.format('jdbc').options(\n",
    "    url=url,\n",
    "    driver=driver,\n",
    "    dbtable='CDW_SAPP_BRANCH',\n",
    "    user=user,\n",
    "    password=password).load()\n",
    "\n",
    "crdt_df = spark.read.format('jdbc').options(\n",
    "    url=url,\n",
    "    driver=driver,\n",
    "    dbtable='CDW_SAPP_CREDIT_CARD',\n",
    "    user=user,\n",
    "    password=password).load()\n",
    "\n",
    "cstmr_df = spark.read.format('jdbc').options(\n",
    "    url=url,\n",
    "    driver=driver,\n",
    "    dbtable='CDW_SAPP_CUSTOMER',\n",
    "    user=user,\n",
    "    password=password).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "brnch_df = brnch_df.toPandas()\n",
    "crdt_df = crdt_df.toPandas()\n",
    "cstmr_df = cstmr_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtled_cstmr = pd.merge(cstmr_df, crdt_df, on='CREDIT_CARD_NO', how='inner')\n",
    "dtled_brnch = pd.merge(brnch_df, crdt_df, on='BRANCH_CODE', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CREDIT_CARD_NO</th>\n",
       "      <th>CUST_CITY</th>\n",
       "      <th>CUST_COUNTRY</th>\n",
       "      <th>CUST_EMAIL</th>\n",
       "      <th>CUST_PHONE</th>\n",
       "      <th>CUST_STATE</th>\n",
       "      <th>CUST_ZIP</th>\n",
       "      <th>FIRST_NAME</th>\n",
       "      <th>LAST_NAME</th>\n",
       "      <th>LAST_UPDATED</th>\n",
       "      <th>MIDDLE_NAME</th>\n",
       "      <th>SSN</th>\n",
       "      <th>FULL_STREET_ADDRESS</th>\n",
       "      <th>BRANCH_CODE</th>\n",
       "      <th>CUST_SSN</th>\n",
       "      <th>TRANSACTION_ID</th>\n",
       "      <th>TRANSACTION_TYPE</th>\n",
       "      <th>TRANSACTION_VALUE</th>\n",
       "      <th>TIMEID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4210653310061055</td>\n",
       "      <td>Natchez</td>\n",
       "      <td>United States</td>\n",
       "      <td>AHooper@example.com</td>\n",
       "      <td>123-7818</td>\n",
       "      <td>MS</td>\n",
       "      <td>39120</td>\n",
       "      <td>Alec</td>\n",
       "      <td>Hooper</td>\n",
       "      <td>2018-04-21 09:49:02</td>\n",
       "      <td>Wm</td>\n",
       "      <td>123456100</td>\n",
       "      <td>Main Street North, 656</td>\n",
       "      <td>59</td>\n",
       "      <td>123456100</td>\n",
       "      <td>20528</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>46.60</td>\n",
       "      <td>2018-12-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4210653310061055</td>\n",
       "      <td>Natchez</td>\n",
       "      <td>United States</td>\n",
       "      <td>AHooper@example.com</td>\n",
       "      <td>123-7818</td>\n",
       "      <td>MS</td>\n",
       "      <td>39120</td>\n",
       "      <td>Alec</td>\n",
       "      <td>Hooper</td>\n",
       "      <td>2018-04-21 09:49:02</td>\n",
       "      <td>Wm</td>\n",
       "      <td>123456100</td>\n",
       "      <td>Main Street North, 656</td>\n",
       "      <td>145</td>\n",
       "      <td>123456100</td>\n",
       "      <td>20529</td>\n",
       "      <td>Gas</td>\n",
       "      <td>80.30</td>\n",
       "      <td>2018-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4210653310061055</td>\n",
       "      <td>Natchez</td>\n",
       "      <td>United States</td>\n",
       "      <td>AHooper@example.com</td>\n",
       "      <td>123-7818</td>\n",
       "      <td>MS</td>\n",
       "      <td>39120</td>\n",
       "      <td>Alec</td>\n",
       "      <td>Hooper</td>\n",
       "      <td>2018-04-21 09:49:02</td>\n",
       "      <td>Wm</td>\n",
       "      <td>123456100</td>\n",
       "      <td>Main Street North, 656</td>\n",
       "      <td>58</td>\n",
       "      <td>123456100</td>\n",
       "      <td>20530</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>27.69</td>\n",
       "      <td>2018-05-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4210653310061055</td>\n",
       "      <td>Natchez</td>\n",
       "      <td>United States</td>\n",
       "      <td>AHooper@example.com</td>\n",
       "      <td>123-7818</td>\n",
       "      <td>MS</td>\n",
       "      <td>39120</td>\n",
       "      <td>Alec</td>\n",
       "      <td>Hooper</td>\n",
       "      <td>2018-04-21 09:49:02</td>\n",
       "      <td>Wm</td>\n",
       "      <td>123456100</td>\n",
       "      <td>Main Street North, 656</td>\n",
       "      <td>93</td>\n",
       "      <td>123456100</td>\n",
       "      <td>20531</td>\n",
       "      <td>Grocery</td>\n",
       "      <td>42.57</td>\n",
       "      <td>2018-06-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4210653310061055</td>\n",
       "      <td>Natchez</td>\n",
       "      <td>United States</td>\n",
       "      <td>AHooper@example.com</td>\n",
       "      <td>123-7818</td>\n",
       "      <td>MS</td>\n",
       "      <td>39120</td>\n",
       "      <td>Alec</td>\n",
       "      <td>Hooper</td>\n",
       "      <td>2018-04-21 09:49:02</td>\n",
       "      <td>Wm</td>\n",
       "      <td>123456100</td>\n",
       "      <td>Main Street North, 656</td>\n",
       "      <td>8</td>\n",
       "      <td>123456100</td>\n",
       "      <td>20532</td>\n",
       "      <td>Test</td>\n",
       "      <td>26.96</td>\n",
       "      <td>2018-06-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     CREDIT_CARD_NO CUST_CITY   CUST_COUNTRY           CUST_EMAIL CUST_PHONE  \\\n",
       "0  4210653310061055   Natchez  United States  AHooper@example.com   123-7818   \n",
       "1  4210653310061055   Natchez  United States  AHooper@example.com   123-7818   \n",
       "2  4210653310061055   Natchez  United States  AHooper@example.com   123-7818   \n",
       "3  4210653310061055   Natchez  United States  AHooper@example.com   123-7818   \n",
       "4  4210653310061055   Natchez  United States  AHooper@example.com   123-7818   \n",
       "\n",
       "  CUST_STATE  CUST_ZIP FIRST_NAME LAST_NAME        LAST_UPDATED MIDDLE_NAME  \\\n",
       "0         MS     39120       Alec    Hooper 2018-04-21 09:49:02          Wm   \n",
       "1         MS     39120       Alec    Hooper 2018-04-21 09:49:02          Wm   \n",
       "2         MS     39120       Alec    Hooper 2018-04-21 09:49:02          Wm   \n",
       "3         MS     39120       Alec    Hooper 2018-04-21 09:49:02          Wm   \n",
       "4         MS     39120       Alec    Hooper 2018-04-21 09:49:02          Wm   \n",
       "\n",
       "         SSN     FULL_STREET_ADDRESS  BRANCH_CODE   CUST_SSN  TRANSACTION_ID  \\\n",
       "0  123456100  Main Street North, 656           59  123456100           20528   \n",
       "1  123456100  Main Street North, 656          145  123456100           20529   \n",
       "2  123456100  Main Street North, 656           58  123456100           20530   \n",
       "3  123456100  Main Street North, 656           93  123456100           20531   \n",
       "4  123456100  Main Street North, 656            8  123456100           20532   \n",
       "\n",
       "  TRANSACTION_TYPE  TRANSACTION_VALUE     TIMEID  \n",
       "0    Entertainment              46.60 2018-12-05  \n",
       "1              Gas              80.30 2018-07-01  \n",
       "2       Healthcare              27.69 2018-05-17  \n",
       "3          Grocery              42.57 2018-06-07  \n",
       "4             Test              26.96 2018-06-11  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtled_cstmr.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRANCH_CITY</th>\n",
       "      <th>BRANCH_CODE</th>\n",
       "      <th>BRANCH_NAME</th>\n",
       "      <th>BRANCH_PHONE</th>\n",
       "      <th>BRANCH_STATE</th>\n",
       "      <th>BRANCH_STREET</th>\n",
       "      <th>BRANCH_ZIP</th>\n",
       "      <th>LAST_UPDATED</th>\n",
       "      <th>CREDIT_CARD_NO</th>\n",
       "      <th>CUST_SSN</th>\n",
       "      <th>TRANSACTION_ID</th>\n",
       "      <th>TRANSACTION_TYPE</th>\n",
       "      <th>TRANSACTION_VALUE</th>\n",
       "      <th>TIMEID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lakeville</td>\n",
       "      <td>1</td>\n",
       "      <td>Example Bank</td>\n",
       "      <td>(123)456-5276</td>\n",
       "      <td>MN</td>\n",
       "      <td>Bridle Court</td>\n",
       "      <td>55044</td>\n",
       "      <td>2018-04-18 13:51:47</td>\n",
       "      <td>4210653349028689</td>\n",
       "      <td>123459988</td>\n",
       "      <td>77</td>\n",
       "      <td>Bills</td>\n",
       "      <td>53.03</td>\n",
       "      <td>2018-11-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lakeville</td>\n",
       "      <td>1</td>\n",
       "      <td>Example Bank</td>\n",
       "      <td>(123)456-5276</td>\n",
       "      <td>MN</td>\n",
       "      <td>Bridle Court</td>\n",
       "      <td>55044</td>\n",
       "      <td>2018-04-18 13:51:47</td>\n",
       "      <td>4210653312528499</td>\n",
       "      <td>123459918</td>\n",
       "      <td>239</td>\n",
       "      <td>Education</td>\n",
       "      <td>58.78</td>\n",
       "      <td>2018-11-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lakeville</td>\n",
       "      <td>1</td>\n",
       "      <td>Example Bank</td>\n",
       "      <td>(123)456-5276</td>\n",
       "      <td>MN</td>\n",
       "      <td>Bridle Court</td>\n",
       "      <td>55044</td>\n",
       "      <td>2018-04-18 13:51:47</td>\n",
       "      <td>4210653312528499</td>\n",
       "      <td>123459918</td>\n",
       "      <td>246</td>\n",
       "      <td>Bills</td>\n",
       "      <td>5.49</td>\n",
       "      <td>2018-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lakeville</td>\n",
       "      <td>1</td>\n",
       "      <td>Example Bank</td>\n",
       "      <td>(123)456-5276</td>\n",
       "      <td>MN</td>\n",
       "      <td>Bridle Court</td>\n",
       "      <td>55044</td>\n",
       "      <td>2018-04-18 13:51:47</td>\n",
       "      <td>4210653352364608</td>\n",
       "      <td>123451215</td>\n",
       "      <td>45319</td>\n",
       "      <td>Bills</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2018-08-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lakeville</td>\n",
       "      <td>1</td>\n",
       "      <td>Example Bank</td>\n",
       "      <td>(123)456-5276</td>\n",
       "      <td>MN</td>\n",
       "      <td>Bridle Court</td>\n",
       "      <td>55044</td>\n",
       "      <td>2018-04-18 13:51:47</td>\n",
       "      <td>4210653362677974</td>\n",
       "      <td>123451185</td>\n",
       "      <td>45423</td>\n",
       "      <td>Bills</td>\n",
       "      <td>2.12</td>\n",
       "      <td>2018-05-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  BRANCH_CITY  BRANCH_CODE   BRANCH_NAME   BRANCH_PHONE BRANCH_STATE  \\\n",
       "0   Lakeville            1  Example Bank  (123)456-5276           MN   \n",
       "1   Lakeville            1  Example Bank  (123)456-5276           MN   \n",
       "2   Lakeville            1  Example Bank  (123)456-5276           MN   \n",
       "3   Lakeville            1  Example Bank  (123)456-5276           MN   \n",
       "4   Lakeville            1  Example Bank  (123)456-5276           MN   \n",
       "\n",
       "  BRANCH_STREET  BRANCH_ZIP        LAST_UPDATED    CREDIT_CARD_NO   CUST_SSN  \\\n",
       "0  Bridle Court       55044 2018-04-18 13:51:47  4210653349028689  123459988   \n",
       "1  Bridle Court       55044 2018-04-18 13:51:47  4210653312528499  123459918   \n",
       "2  Bridle Court       55044 2018-04-18 13:51:47  4210653312528499  123459918   \n",
       "3  Bridle Court       55044 2018-04-18 13:51:47  4210653352364608  123451215   \n",
       "4  Bridle Court       55044 2018-04-18 13:51:47  4210653362677974  123451185   \n",
       "\n",
       "   TRANSACTION_ID TRANSACTION_TYPE  TRANSACTION_VALUE     TIMEID  \n",
       "0              77            Bills              53.03 2018-11-28  \n",
       "1             239        Education              58.78 2018-11-26  \n",
       "2             246            Bills               5.49 2018-04-04  \n",
       "3           45319            Bills               1.68 2018-08-02  \n",
       "4           45423            Bills               2.12 2018-05-12  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtled_brnch.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Transaction Details Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipTransactions(z, m, y):\n",
    "    df = dtled_cstmr[(dtled_cstmr['CUST_ZIP'] == z) & (dtled_cstmr['TIMEID'].dt.month == m) & (dtled_cstmr['TIMEID'].dt.year == y)]\n",
    "    df = df.sort_values(by = 'TIMEID', ascending = False)\n",
    "\n",
    "    df = df[['FIRST_NAME', 'LAST_NAME', 'CUST_ZIP', 'TRANSACTION_VALUE', 'TIMEID']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typeTransaction(tType):\n",
    "    return crdt_df[crdt_df['TRANSACTION_TYPE'] == tType]['TRANSACTION_VALUE'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stateTransaction(state):\n",
    "    fltr = dtled_brnch[dtled_brnch['BRANCH_STATE'] == state].groupby('BRANCH_CODE')['TRANSACTION_VALUE'].sum()\n",
    "    fltr = fltr.to_frame()\n",
    "    fltr = fltr.reset_index()\n",
    "\n",
    "    fltr.columns = ['BRANCH ID', 'TOTAL TRANSACTIONS']\n",
    "    \n",
    "    return fltr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Customer Details Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkAccount(cardId):\n",
    "    return cstmr_df[cstmr_df['CREDIT_CARD_NO'] == cardId].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateAccount(cardId, name):\n",
    "    global cstmr_df\n",
    "    df = cstmr_df\n",
    "    \n",
    "    df.loc[cstmr_df['CREDIT_CARD_NO'] == cardId, 'FIRST_NAME'] == name\n",
    "    df = spark.createDataFrame(df)\n",
    "\n",
    "    df.write.format('jdbc').options(\n",
    "        url='jdbc:mysql://localhost:3306/creditcard_capstone?permitMysqlScheme',\n",
    "        driver='org.mariadb.jdbc.Driver',\n",
    "        dbtable='cdw_sapp_customer',\n",
    "        user='root',\n",
    "        password='123456').mode('append').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBill(cardId, m, yr):\n",
    "    df = dtled_cstmr[(dtled_cstmr['CREDIT_CARD_NO'] == cardId) & (dtled_cstmr['TIMEID'].dt.month == m) & (dtled_cstmr['TIMEID'].dt.year == yr)]['TRANSACTION_VALUE'].sum()\n",
    "    \n",
    "    return df.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transactionHistory(cardId, start, end):\n",
    "    return dtled_cstmr[(dtled_cstmr['CREDIT_CARD_NO'] == cardId) & (dtled_cstmr['TIMEID'].between(start, end))][[ 'TIMEID', 'TRANSACTION_TYPE', 'TRANSACTION_VALUE']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Console Program Starts Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Perscholas Banking Terminal\n",
      "\n",
      "What would you like to do? Type 1 for Transaction Details. Type 2 for Customer Details\n",
      "You have chosen: Customer Details\n",
      "\n",
      "Choose the Following Options: Type 1 to Check Account Details. Type 2 to Update Account Details. Type 3 for Create Monthly Bill. Type 4 to See Transaction History\n",
      "You gave chosen: Update Account\n",
      "\n",
      "Please Enter Your Credit Card Number: \n",
      "\n",
      "Change Your First Name\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o306.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 17.0 failed 1 times, most recent failure: Lost task 10.0 in stage 17.0 (TID 49) (DESKTOP-6JUSJFI executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 35 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:867)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 35 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-af427bb86ca4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdateAccount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcardId\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Action Completed'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-94-52d78f016774>\u001b[0m in \u001b[0;36mupdateAccount\u001b[1;34m(cardId, name)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     df.write.format('jdbc').options(\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'jdbc:mysql://localhost:3306/creditcard_capstone?permitMysqlScheme'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mdriver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'org.mariadb.jdbc.Driver'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\datur\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    964\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\datur\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\datur\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\datur\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o306.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 17.0 failed 1 times, most recent failure: Lost task 10.0 in stage 17.0 (TID 49) (DESKTOP-6JUSJFI executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 35 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:867)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:535)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:189)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 35 more\r\n"
     ]
    }
   ],
   "source": [
    "print('Welcome to Perscholas Banking Terminal')\n",
    "\n",
    "print('')\n",
    "\n",
    "print('What would you like to do? Type 1 for Transaction Details. Type 2 for Customer Details')\n",
    "\n",
    "choice = int(input('Choose Your Options (1 or 2): '))\n",
    "\n",
    "if choice == 1:\n",
    "    print('You have chosen: Transaction Details')\n",
    "\n",
    "    print('')\n",
    "\n",
    "    print('Choose the Following Options: Type 1 for Customer Transaction Details by Zip Code. Type 2 for Transaction Details by Type. Type 3 for Branch Transaction Details by State')\n",
    "\n",
    "    choice2 = int(input('Choose Your Options (1, 2 or 3): '))\n",
    "\n",
    "    if choice2 == 1:\n",
    "        print('You have chosen: Customer Transaction Details by Zip Code.')\n",
    "\n",
    "        print('')\n",
    "\n",
    "        print('Please Enter the Information in the Following Order: Zip Code, Month and Year.')\n",
    "        \n",
    "        zpcd = int(input('Desired Zip Code: '))\n",
    "        m = int(input('Desired Month: '))\n",
    "        y = int(input('Desired Year: '))\n",
    "\n",
    "        print('Inputs Recieved: ' + str(zpcd) + ' ' + str(m) + ' ' + str(y))\n",
    "\n",
    "        print('')\n",
    "\n",
    "        print('Generating Results')\n",
    "\n",
    "        print('Transactions in the Zip Code: ' + str(zpcd) + ', the Month: ' + str(m) + ', and the Year: ' + str(y) + ', in Decending Order: ')\n",
    "\n",
    "        print('')\n",
    "        \n",
    "        print(zipTransactions(zpcd, m, y))\n",
    "\n",
    "    elif choice2 == 2:\n",
    "        print('You have chosen: Transaction Details by Type')\n",
    "\n",
    "        print('')\n",
    "\n",
    "        print('Please Enter What Type of Transaction You Want to See: ')\n",
    "\n",
    "        typ = input('Desired Type: ')\n",
    "\n",
    "        print('Type Chosen: ' + '\"' + typ + '\"')\n",
    "\n",
    "        print('')\n",
    "\n",
    "        print('Generating Results')\n",
    "\n",
    "        print('Total Transactions for ' + '\"' + typ + '\"'+ ' is: '+ str(typeTransaction(typ)))\n",
    "\n",
    "    elif choice2 == 3:\n",
    "        print('You have chosen: Branch Transaction Details by State')\n",
    "        \n",
    "        print('')\n",
    "\n",
    "        print('Please Enter What State You Want: ')\n",
    "\n",
    "        state = input('Desired State: ')\n",
    "\n",
    "        print('State Chosen: ' + '\"' + state + '\"')\n",
    "\n",
    "        print('')\n",
    "\n",
    "        print('Generating Results')\n",
    "\n",
    "        print('Total Transactions in ' + '\"' + state + '\"'+ ' by Branches: ')\n",
    "\n",
    "        print('')\n",
    "\n",
    "        print(stateTransaction(state))\n",
    "        \n",
    "    else:\n",
    "        print('Try Again')\n",
    "\n",
    "elif choice == 2:\n",
    "    print('You have chosen: Customer Details')\n",
    "    \n",
    "    print('')\n",
    "\n",
    "    print('Choose the Following Options: Type 1 to Check Account Details. Type 2 to Update Account Details. Type 3 for Create Monthly Bill. Type 4 to See Transaction History')\n",
    "\n",
    "    choice3 = int(input('Choose Your Options (1, 2, 3 or 4): '))\n",
    "\n",
    "    if choice3 == 1:\n",
    "        print('You have chosen: Check Account Details')\n",
    "        \n",
    "        print('')\n",
    "        \n",
    "        print('Please Enter in Your Credit Card Number: ')\n",
    "\n",
    "        cardId = input('Credit Card Number: ')\n",
    "\n",
    "        print('')\n",
    "\n",
    "        print('Getting Account Details')\n",
    "\n",
    "        print('')\n",
    "\n",
    "        print(checkAccount(cardId))\n",
    "\n",
    "    elif choice3 == 2:\n",
    "        print('You gave chosen: Update Account')\n",
    "\n",
    "        print('')\n",
    "\n",
    "        print('Please Enter Your Credit Card Number: ')\n",
    "        cardId = input('Credit Card Number: ')\n",
    "        \n",
    "        print('')\n",
    "\n",
    "        print('Change Your First Name')\n",
    "        name = input('Your First Name: ')\n",
    "\n",
    "        print('')\n",
    "\n",
    "        print(updateAccount(cardId, name))\n",
    "\n",
    "        print('Action Completed')\n",
    "\n",
    "        print('')\n",
    "\n",
    "        print('Have a Nice Day!')\n",
    "    \n",
    "    elif choice3 == 3:\n",
    "        print('You have chosen: Check Monthly Bill')\n",
    "        \n",
    "        print('')\n",
    "        \n",
    "        print('Please Enter Your Credit Card Number: ')\n",
    "\n",
    "        cardId = input('Credit Card Number: ')\n",
    "        m = int(input('Desired Month: '))\n",
    "        y = int(input('Desired Year: '))\n",
    "\n",
    "        print('Generating Monthly Bill')\n",
    "\n",
    "        print('')\n",
    "        \n",
    "        print('Amount Due: ')\n",
    "\n",
    "        print(createBill(cardId, m, y))\n",
    "\n",
    "        print('')\n",
    "\n",
    "        print('Have a Nice Day!')\n",
    "        \n",
    "    elif choice3 == 4:\n",
    "        print('You have chosen: Transaction History')\n",
    "        \n",
    "        print('')\n",
    "        \n",
    "        print('Please Enter Your Credit Card Number: ')\n",
    "\n",
    "        cardId = input('Credit Card Number: ')\n",
    "\n",
    "        print('')\n",
    "\n",
    "        print('Please the Start and End Dates: ')\n",
    "\n",
    "        s = input('Start Date (Year-Month-Day): ')\n",
    "        e = input('End Date (Year-Month-Day): ')\n",
    "\n",
    "        print('Generating Transaction History')\n",
    "\n",
    "        print('')\n",
    "    \n",
    "        print(transactionHistory(cardId, s, e))\n",
    "\n",
    "    else:\n",
    "        print('Try Again')\n",
    "\n",
    "else:\n",
    "    print('Try Again')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CREDIT_CARD_NO</th>\n",
       "      <th>CUST_CITY</th>\n",
       "      <th>CUST_COUNTRY</th>\n",
       "      <th>CUST_EMAIL</th>\n",
       "      <th>CUST_PHONE</th>\n",
       "      <th>CUST_STATE</th>\n",
       "      <th>CUST_ZIP</th>\n",
       "      <th>FIRST_NAME</th>\n",
       "      <th>LAST_NAME</th>\n",
       "      <th>LAST_UPDATED</th>\n",
       "      <th>MIDDLE_NAME</th>\n",
       "      <th>SSN</th>\n",
       "      <th>FULL_STREET_ADDRESS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4210653310195948</td>\n",
       "      <td>NewBerlin</td>\n",
       "      <td>United States</td>\n",
       "      <td>EHardy@example.com</td>\n",
       "      <td>124-3215</td>\n",
       "      <td>WI</td>\n",
       "      <td>53151</td>\n",
       "      <td>Eugenio</td>\n",
       "      <td>Hardy</td>\n",
       "      <td>2018-04-21 09:49:02</td>\n",
       "      <td>Trina</td>\n",
       "      <td>123459758</td>\n",
       "      <td>Country Club Road, 253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>4210653310195948</td>\n",
       "      <td>NewBerlin</td>\n",
       "      <td>United States</td>\n",
       "      <td>EHardy@example.com</td>\n",
       "      <td>124-3215</td>\n",
       "      <td>WI</td>\n",
       "      <td>53151</td>\n",
       "      <td>Eugenio</td>\n",
       "      <td>Hardy</td>\n",
       "      <td>2018-04-21 09:49:02</td>\n",
       "      <td>Trina</td>\n",
       "      <td>123459758</td>\n",
       "      <td>Country Club Road, 253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>4210653310195948</td>\n",
       "      <td>NewBerlin</td>\n",
       "      <td>United States</td>\n",
       "      <td>EHardy@example.com</td>\n",
       "      <td>124-3215</td>\n",
       "      <td>WI</td>\n",
       "      <td>53151</td>\n",
       "      <td>Eugenio</td>\n",
       "      <td>Hardy</td>\n",
       "      <td>2018-04-21 09:49:02</td>\n",
       "      <td>Trina</td>\n",
       "      <td>123459758</td>\n",
       "      <td>Country Club Road, 253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CREDIT_CARD_NO  CUST_CITY   CUST_COUNTRY          CUST_EMAIL  \\\n",
       "3     4210653310195948  NewBerlin  United States  EHardy@example.com   \n",
       "955   4210653310195948  NewBerlin  United States  EHardy@example.com   \n",
       "1907  4210653310195948  NewBerlin  United States  EHardy@example.com   \n",
       "\n",
       "     CUST_PHONE CUST_STATE  CUST_ZIP FIRST_NAME LAST_NAME        LAST_UPDATED  \\\n",
       "3      124-3215         WI     53151    Eugenio     Hardy 2018-04-21 09:49:02   \n",
       "955    124-3215         WI     53151    Eugenio     Hardy 2018-04-21 09:49:02   \n",
       "1907   124-3215         WI     53151    Eugenio     Hardy 2018-04-21 09:49:02   \n",
       "\n",
       "     MIDDLE_NAME        SSN     FULL_STREET_ADDRESS  \n",
       "3          Trina  123459758  Country Club Road, 253  \n",
       "955        Trina  123459758  Country Club Road, 253  \n",
       "1907       Trina  123459758  Country Club Road, 253  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cstmr_df[cstmr_df['CREDIT_CARD_NO'] == '4210653310195948']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "268b35713270089dce1907f2bdf2b5330d291c9010b4c5ccc413b7aaa71d7709"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
